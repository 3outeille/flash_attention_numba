# flash_attention_numba

- `pytest test.py -v` to run the tests

# TODO:

- [ ] divide by sqrt(d_k)
- [ ] Binding numba to torch
- [ ] Bench against torch & Cpp flash attention